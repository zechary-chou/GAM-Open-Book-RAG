#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GAM æ¡†æ¶ + NarrativeQA æ•°æ®é›†æµ‹è¯•æ–‡ä»¶

åŸºäº test_gam_hotpotqa.pyï¼Œé€‚é… NarrativeQA æ•°æ®é›†æ ¼å¼ã€‚
NarrativeQA æ•°æ®æ ¼å¼ï¼š
- document: dict - åŒ…å«æ–‡æ¡£ä¿¡æ¯
  - text: str - é•¿æ–‡æœ¬ï¼ˆéœ€è¦åˆ‡åˆ†ï¼‰
  - id: str - æ–‡æ¡£ID
  - summary: dict - æ‘˜è¦ä¿¡æ¯
- question: dict - åŒ…å«é—®é¢˜
  - text: str - é—®é¢˜æ–‡æœ¬
- answers: List[dict] - ç­”æ¡ˆåˆ—è¡¨ï¼Œæ¯ä¸ªç­”æ¡ˆæœ‰ text å­—æ®µ
"""

import string
import sys
import os
import re
import json
import random
from typing import Any, Counter, Dict, List, Optional, Tuple
from tqdm import tqdm


from gam import (
    MemoryAgent,
    ResearchAgent,
    VLLMGenerator,
    OpenAIGenerator,
    OpenAIGeneratorConfig,
    InMemoryMemoryStore,
    InMemoryPageStore,
    IndexRetriever,
    BM25Retriever,
    DenseRetriever,
    VLLMGeneratorConfig,
    IndexRetrieverConfig,
    BM25RetrieverConfig,
    DenseRetrieverConfig,
)

# ========== æ•°æ®åŠ è½½ ==========

def load_narrativeqa(data_dir: str, split: str = "test") -> List[Dict[str, Any]]:
    """
    åŠ è½½ NarrativeQA æ•°æ®é›†
    
    Args:
        data_dir: æ•°æ®é›†ç›®å½•è·¯å¾„ï¼ˆåŒ…å« parquet æ–‡ä»¶çš„ç›®å½•ï¼‰
        split: æ•°æ®é›†åˆ†å‰²ï¼ˆ"train", "validation", "test"ï¼‰
    """
    from datasets import load_dataset
    
    # åŠ è½½æ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†: {data_dir}, {split}")
    dataset = load_dataset("parquet", data_dir=data_dir, split=split)
    
    print(f"åŠ è½½æˆåŠŸï¼Œæ•°æ®é›†é•¿åº¦: {len(dataset)}")

    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
    data_all = []
    for idx, item in enumerate(dataset):
        # æå–æ–‡æ¡£æ–‡æœ¬
        document = item.get("document", {})
        document_text = document.get("text", "") if isinstance(document, dict) else ""
        document_id = document.get("id", f"doc-{idx}") if isinstance(document, dict) else f"doc-{idx}"
        
        # æå–é—®é¢˜æ–‡æœ¬
        question = item.get("question", {})
        question_text = question.get("text", "") if isinstance(question, dict) else ""
        
        # æå–ç­”æ¡ˆåˆ—è¡¨ï¼ˆä» answers åˆ—è¡¨ä¸­æå– text å­—æ®µï¼‰
        answers_raw = item.get("answers", [])
        answers = []
        if isinstance(answers_raw, list):
            for ans in answers_raw:
                if isinstance(ans, dict):
                    ans_text = ans.get("text", "")
                    if ans_text:
                        answers.append(ans_text)
                elif isinstance(ans, str):
                    answers.append(ans)
        
        data_all.append({
            "index": idx,
            "document_text": document_text,
            "document_id": document_id,
            "question": question_text,
            "answers": answers,
            "_id": f"narrativeqa-{document_id}-{idx}"  # ç”Ÿæˆå”¯ä¸€ID
        })
    
    return data_all

# ========== é•¿æ–‡æœ¬åˆ‡åˆ† ==========

def build_context_chunks_for_sample(
    sample: Dict[str, Any], 
    max_tokens: int = 2000, 
    embedding_model_path: Optional[str] = None
) -> List[str]:
    """
    å°† document_text æ–‡æœ¬æŒ‰ token æ•°é‡åˆ†å‰²æˆå¤šä¸ªä¼šè¯å—
    ä½¿ç”¨æ™ºèƒ½åˆ‡åˆ†ï¼šä¼˜å…ˆåœ¨è¾¹ç•Œå¤„åˆ‡åˆ†
    
    Args:
        sample: æ ·æœ¬æ•°æ®ï¼ŒåŒ…å« 'document_text' å­—æ®µ
        max_tokens: æ¯ä¸ªä¼šè¯å—çš„æœ€å¤§ token æ•°é‡
        embedding_model_path: embedding æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œç²¾ç¡® token è®¡ç®—
    """
    context_text = sample.get("document_text") or ""
    
    if not context_text:
        return []
    
    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    if embedding_model_path:
        try:
            chunks = _split_with_embedding_model(context_text, max_tokens, embedding_model_path)
            if chunks:
                return chunks
        except Exception as e:
            print(f"Warning: Embedding model splitting failed: {e}, falling back to tiktoken")
    
    # ä½¿ç”¨ tiktoken è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    try:
        import tiktoken
        tokenizer = tiktoken.encoding_for_model("gpt-4o-2024-08-06")
        tokens = tokenizer.encode(context_text, disallowed_special=())
        
        if len(tokens) <= max_tokens:
            return [f"[Session 1]\n{context_text}"]
        
        # æ™ºèƒ½åˆ‡åˆ†ï¼šæŒ‰ token æ•°é‡åˆ‡åˆ†
        chunks = _smart_split_by_tokens(context_text, tokens, max_tokens, tokenizer)
        return chunks
        
    except ImportError:
        print("Warning: tiktoken not available, falling back to character-based splitting")
        return _fallback_char_split(context_text, max_tokens)

def _split_with_embedding_model(text: str, max_tokens: int, model_path: str) -> List[str]:
    """
    ä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    """
    try:
        from transformers import AutoTokenizer
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # ç¼–ç æ–‡æœ¬è·å– tokens
        tokens = tokenizer.encode(text, add_special_tokens=False)
        
        if len(tokens) <= max_tokens:
            return [f"[Session 1]\n{text}"]
        
        # æ™ºèƒ½åˆ‡åˆ†
        chunks = _smart_split_by_tokens(text, tokens, max_tokens, tokenizer)
        return chunks
        
    except Exception as e:
        print(f"Error using embedding model: {e}")
        return []

def _smart_split_by_tokens(text: str, tokens: List[int], max_tokens: int, tokenizer) -> List[str]:
    """
    æŒ‰ token æ•°é‡ç®€å•åˆ‡åˆ†ï¼šä¸è¿›è¡Œæ™ºèƒ½è¾¹ç•ŒæŸ¥æ‰¾ï¼Œç›´æ¥æŒ‰ max_tokens åˆ‡åˆ†
    """
    chunks = []
    
    # å¦‚æœæ–‡æœ¬ä¸è¶…è¿‡æœ€å¤§ token æ•°ï¼Œç›´æ¥è¿”å›
    if len(tokens) <= max_tokens:
        return [f"[Session 1]\n{text}"]
    
    # ç›´æ¥æŒ‰ç…§ token ç´¢å¼•åˆ‡åˆ†
    session_id = 1
    start_idx = 0
    
    while start_idx < len(tokens):
        # è®¡ç®—å½“å‰å—çš„ç»“æŸ token ç´¢å¼•
        end_idx = min(start_idx + max_tokens, len(tokens))
        
        # å°† tokens è§£ç å›æ–‡æœ¬
        chunk_tokens = tokens[start_idx:end_idx]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        
        if chunk_text.strip():
            chunks.append(f"[Session {session_id}]\n{chunk_text.strip()}")
            session_id += 1
        
        start_idx = end_idx
    
    return chunks

def _fallback_char_split(text: str, max_tokens: int) -> List[str]:
    """
    å­—ç¬¦åˆ‡åˆ†çš„ fallback æ–¹æ³•
    """
    # ç²—ç•¥ä¼°è®¡ï¼š1 token â‰ˆ 4 characters
    max_chars = max_tokens * 4
    
    if len(text) <= max_chars:
        return [f"[Session 1]\n{text}"]
    
    chunks = []
    current_start = 0
    session_id = 1
    
    while current_start < len(text):
        current_end = min(current_start + max_chars, len(text))
        
        # å°è¯•åœ¨å•è¯è¾¹ç•Œåˆ‡åˆ†
        if current_end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œç¬¦
            last_newline = text.rfind('\n', current_start, current_end)
            if last_newline > current_start:
                current_end = last_newline
            else:
                # å¯»æ‰¾æœ€åä¸€ä¸ªç©ºæ ¼
                last_space = text.rfind(' ', current_start, current_end)
                if last_space > current_start:
                    current_end = last_space
        
        chunk_text = text[current_start:current_end].strip()
        if chunk_text:
            chunks.append(f"[Session {session_id}]\n{chunk_text}")
            session_id += 1
        
        current_start = current_end
    
    return chunks

# ========== Prompt è®¾è®¡ ==========

def make_prompt(summary: str, question: str) -> str:
    """åˆ›å»ºç»Ÿä¸€çš„ Promptï¼ˆå¼€æ”¾é—®ç­”æ ¼å¼ï¼‰"""
    prompt = f"""You are a careful reading assistant. 
Use the given Context. 
Answer with ONLY the final answer string; no extra words.

Question:
{question}

Context:
{summary}

Answer:
"""
    return prompt

# ========== ç­”æ¡ˆæå–å’Œè¯„ä¼° ==========
def normalize_answer(s):
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)
    def white_space_fix(text):
        return " ".join(text.split())
    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)
    def lower(text):
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(s))))

def f1_score(prediction, ground_truth, **kwargs):
    common = Counter(prediction) & Counter(ground_truth)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction)
    recall = 1.0 * num_same / len(ground_truth)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def qa_f1_score(prediction, ground_truth, **kwargs):
    normalized_prediction = normalize_answer(prediction)
    normalized_ground_truth = normalize_answer(ground_truth)
    prediction_tokens = normalized_prediction.split()
    ground_truth_tokens = normalized_ground_truth.split()
    return f1_score(prediction_tokens, ground_truth_tokens)

def _calculate_f1(pred_answer: str, gold_answers: List[str]) -> float:
    # è®¡ç®—ä¸æ¯ä¸ªæ ‡å‡†ç­”æ¡ˆçš„ F1ï¼Œå–æœ€å¤§å€¼
    max_f1 = 0.0
    for gold_answer in gold_answers:
        max_f1 = max(max_f1, qa_f1_score(pred_answer, gold_answer))
    return max_f1

# ========== æ ¸å¿ƒå¤„ç†é€»è¾‘ ==========

def process_sample(
    sample: Dict[str, Any], 
    sample_index: int, 
    outdir: str,
    memory_api_key: str,
    memory_base_url: str,
    memory_model: str,
    research_api_key: str,
    research_base_url: str,
    research_model: str,
    working_api_key: str,
    working_base_url: str,
    working_model: str,
    max_tokens: int = 2000,
    embedding_model_path: Optional[str] = None
):
    """
    ä½¿ç”¨ GAM æ¡†æ¶å¤„ç†å•ä¸ªæ ·æœ¬ã€‚
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†
    2. ä½¿ç”¨ ResearchAgent è¿›è¡Œæ·±åº¦ç ”ç©¶
    3. åŸºäºç ”ç©¶ç»“æœè¿›è¡Œé—®ç­”
    """
    sample_id = sample.get("_id", f"sample-{sample_index}")
    
    print(f"\n{'='*60}")
    print(f"å¤„ç†æ ·æœ¬ #{sample_index}: {sample_id}")
    print(f"{'='*60}")
    
    try:
        # 1. æ„å»ºä¸Šä¸‹æ–‡å—
        context_chunks = build_context_chunks_for_sample(sample, max_tokens, embedding_model_path)
        print(f"ä¸Šä¸‹æ–‡å—æ•°: {len(context_chunks)}")
        if context_chunks:
            print(f"ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡å—é¢„è§ˆ:\n{context_chunks[0][:400]}...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sample_results_dir = os.path.join(outdir, sample_id)
        os.makedirs(sample_results_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {sample_results_dir}")
        
        # 2. åˆ›å»ºå…±äº«å­˜å‚¨
        memory_store = InMemoryMemoryStore(dir_path=sample_results_dir)
        page_store = InMemoryPageStore(dir_path=sample_results_dir)
        
        # 3. åˆ›å»º Memory Generator
        print(f"\næ­¥éª¤ 1: åˆ›å»º Memory Generator")
        memory_generator_config = OpenAIGeneratorConfig(
            model_name=memory_model,
            api_key=memory_api_key,
            base_url=memory_base_url,
            temperature=0.3,
            max_tokens=256
        )
        memory_generator = OpenAIGenerator(memory_generator_config.__dict__)
        
        print(f"[OK] Memory Generator åˆ›å»ºå®Œæˆ")
        
 

        # 4. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†ï¼ˆå°†æ¯ä¸ª context chunk ä½œä¸ºä¸€æ¡æ¶ˆæ¯ï¼‰
        print(f"\næ­¥éª¤ 2: ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†")
        memory_agent = MemoryAgent(
            memory_store=memory_store,
            page_store=page_store,
            generator=memory_generator,
        )
        
        if not os.path.exists(os.path.join(sample_results_dir, 'memory_state.json')):
            for i, context_chunk in enumerate(context_chunks, 1):
                print(f"  å¤„ç†ä¸Šä¸‹æ–‡å— {i}/{len(context_chunks)}...")
                memory_update = memory_agent.memorize(context_chunk)
        
        # æŸ¥çœ‹æ„å»ºçš„è®°å¿†
        final_state = memory_store.load()
        print(f"[OK] è®°å¿†æ„å»ºå®Œæˆï¼å…± {len(final_state.abstracts)} æ¡è®°å¿†æ‘˜è¦")
        
        # æ˜¾ç¤ºè®°å¿†æ‘˜è¦
        print("\nğŸ“š è®°å¿†æ‘˜è¦:")
        for i, abstract in enumerate(final_state.abstracts, 1):
            print(f"  {i}. {abstract[:100]}...")
        
        # ä¿å­˜è®°å¿†çŠ¶æ€
        memory_state_file = os.path.join(sample_results_dir, "memory_state.json")
        with open(memory_state_file, 'w', encoding='utf-8') as f:
            json.dump(final_state.model_dump(), f, ensure_ascii=False, indent=2)
        print(f"[OK] è®°å¿†çŠ¶æ€å·²ä¿å­˜: {memory_state_file}")
        
        # 5. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆç”¨äº ResearchAgentï¼‰
        print(f"\næ­¥éª¤ 3: åˆ›å»ºæ£€ç´¢å™¨ï¼ˆç”¨äº ResearchAgentï¼‰")
        retrievers = {}
        
        # ç´¢å¼•æ£€ç´¢å™¨
        try:
            page_index_dir = os.path.join(sample_results_dir, "page_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(page_index_dir):
                import shutil
                shutil.rmtree(page_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„é¡µé¢ç´¢å¼•ç›®å½•: {page_index_dir}")
            
            index_config = IndexRetrieverConfig(
                index_dir=page_index_dir
            )
            index_retriever = IndexRetriever(index_config.__dict__)
            index_retriever.build(page_store)
            retrievers["page_index"] = index_retriever
            print(f"[OK] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # BM25 æ£€ç´¢å™¨
        try:
            bm25_index_dir = os.path.join(sample_results_dir, "bm25_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(bm25_index_dir):
                import shutil
                shutil.rmtree(bm25_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ BM25 ç´¢å¼•ç›®å½•: {bm25_index_dir}")
            
            bm25_config = BM25RetrieverConfig(
                index_dir=bm25_index_dir,
                threads=1
            )
            bm25_retriever = BM25Retriever(bm25_config.__dict__)
            bm25_retriever.build(page_store)
            retrievers["keyword"] = bm25_retriever
            print(f"[OK] BM25 æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] BM25 æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # Dense æ£€ç´¢å™¨
        try:
            dense_index_dir = os.path.join(sample_results_dir, "dense_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(dense_index_dir):
                import shutil
                shutil.rmtree(dense_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ Dense ç´¢å¼•ç›®å½•: {dense_index_dir}")
            
            dense_config = DenseRetrieverConfig(
                index_dir=dense_index_dir,
                model_name="BAAI/bge-m3"
            )

            # dense_config = DenseRetrieverConfig(
            #     index_dir=dense_index_dir,
            #     api_url="http://localhost:8001" 
            # )

            
            dense_retriever = DenseRetriever(dense_config.__dict__)
            dense_retriever.build(page_store)
            retrievers["vector"] = dense_retriever
            print(f"[OK] Dense æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] Dense æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        print(f"[INFO] æˆåŠŸåˆ›å»º {len(retrievers)} ä¸ªæ£€ç´¢å™¨")
        
        # 6. åˆ›å»º Research Generator å’Œ Working Generator
        print(f"\næ­¥éª¤ 4: åˆ›å»º Research Generator å’Œ Working Generator")
        research_generator_config = OpenAIGeneratorConfig(
            model_name=research_model,
            api_key=research_api_key,
            base_url=research_base_url,
            temperature=0.3,
            max_tokens=2048
        )
        research_generator = OpenAIGenerator(research_generator_config.__dict__)
        
        working_generator_config = OpenAIGeneratorConfig(
            model_name=working_model,
            api_key=working_api_key,
            base_url=working_base_url,
            temperature=0.3,
            max_tokens=256
        )
        working_generator = OpenAIGenerator(working_generator_config.__dict__)
        
        print(f"[OK] Research Generator å’Œ Working Generator åˆ›å»ºå®Œæˆ")
        
        # 7. åˆ›å»º ResearchAgent
        print(f"\næ­¥éª¤ 5: åˆ›å»º ResearchAgent")
        research_agent = ResearchAgent(
            page_store=page_store,
            memory_store=memory_store,
            retrievers=retrievers,
            generator=research_generator,
            max_iters=3
        )
        print(f"[OK] ResearchAgent åˆ›å»ºå®Œæˆ")
        
        # 8. è¿›è¡Œé—®ç­”
        print(f"\næ­¥éª¤ 6: è¿›è¡Œé—®ç­”")
        
        # æå–é—®é¢˜ä¿¡æ¯
        question = sample.get("question", "")
        gold_answers = sample.get("answers", [])
        
        print(f"é—®é¢˜: {question}")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {gold_answers}")
        
        # ä¿å­˜æ‰€æœ‰æ•°æ®å±æ€§
        result = {
            "_id": sample.get("_id", sample_id),
            "sample_id": sample_id,
            "index": sample.get("index", sample_index),
            "document_id": sample.get("document_id", ""),
            "question": question,
            "answers": gold_answers,
            "gold_answers": gold_answers,  # ä¿ç•™ gold_answers ä»¥ä¾¿å…¼å®¹
        }

        try:
            # ä½¿ç”¨ ResearchAgent è¿›è¡Œç ”ç©¶
            print("æ­£åœ¨è¿›è¡Œæ·±åº¦ç ”ç©¶...")
            research_result = research_agent.research(question)
            research_summary = research_result.integrated_memory
            print(f"[OK] ç ”ç©¶å®Œæˆï¼è¿­ä»£æ¬¡æ•°: {len(research_result.raw_memory.get('iterations', []))}")
            print(f"ç ”ç©¶æ‘˜è¦: {research_summary[:200]}...")
            
            # ä¿å­˜ç ”ç©¶è½¨è¿¹
            research_trace = {
                "question": question,
                "raw_memory": research_result.raw_memory,
                "integrated_memory": research_result.integrated_memory,
                "iterations": research_result.raw_memory.get("iterations", []),
                "search_plans": research_result.raw_memory.get("search_plans", []),
                "reflections": research_result.raw_memory.get("reflections", [])
            }
            
            trace_file = os.path.join(sample_results_dir, "research_trace.json")
            with open(trace_file, 'w', encoding='utf-8') as f:
                json.dump(research_trace, f, ensure_ascii=False, indent=2)
            print(f"[INFO] ç ”ç©¶è½¨è¿¹å·²ä¿å­˜: {trace_file}")
            
            result["research_summary"] = research_summary
            result["research_trace_file"] = trace_file
            
            # ä½¿ç”¨ç»Ÿä¸€çš„ prompt æ ¼å¼ç”Ÿæˆç­”æ¡ˆ
            print("ç”Ÿæˆç­”æ¡ˆ...")
            prompt = make_prompt(research_summary, question)
            response = working_generator.generate_single(prompt=prompt)
            answer_text = response.get("text", "").strip()
            
            print(f"æ¨¡å‹å“åº”: {answer_text[:200]}...")
            
            # æå–ç­”æ¡ˆ

            pred_answer = answer_text
            result["response"] = answer_text
            result["pred"] = pred_answer
                        
            # è®¡ç®— F1 åˆ†æ•°
            f1_score = _calculate_f1(pred_answer, gold_answers) if pred_answer else 0.0
            result["f1"] = f1_score
            
            print(f"é¢„æµ‹ç­”æ¡ˆ: {pred_answer}")
            print(f"æ ‡å‡†ç­”æ¡ˆ: {gold_answers}")
            print(f"F1 åˆ†æ•°: {f1_score:.4f}")
            
        except Exception as e:
            print(f"[ERROR] å¤„ç†é—®é¢˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            result["error"] = str(e)
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(sample_results_dir, "qa_result.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ€»ç»“
        print(f"\n{'='*60}")
        print("å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ ·æœ¬ID: {sample_id}")
        print(f"ä¸Šä¸‹æ–‡å—æ•°: {len(context_chunks)}")
        if final_state:
            print(f"è®°å¿†æ‘˜è¦æ•°: {len(final_state.abstracts)}")
        print(f"é¢„æµ‹ç­”æ¡ˆ: {result.get('pred', 'N/A')}")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {gold_answers}")
        print(f"F1 åˆ†æ•°: {result.get('f1', 0.0):.4f}")
        print(f"ç»“æœä¿å­˜åˆ°: {sample_results_dir}")
        
        return result
        
    except Exception as e:
        error_msg = f"å¤„ç†æ ·æœ¬ {sample_index} æ—¶å‡ºé”™: {str(e)}"
        print(f"ERROR: {error_msg}")
        import traceback
        traceback.print_exc()
        return {
            "sample_id": sample.get("_id", f"sample-{sample_index}"),
            "error": error_msg
        }


# ========== ä¸»å‡½æ•° ==========

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="GAM æ¡†æ¶ + NarrativeQA æ•°æ®é›†æµ‹è¯•")
    parser.add_argument("--data-dir", type=str, default="/path/to/narrativeqa/data", 
                        help="NarrativeQA æ•°æ®é›†ç›®å½•è·¯å¾„")
    parser.add_argument("--split", type=str, default="test", choices=["train", "validation", "test"],
                        help="æ•°æ®é›†åˆ†å‰²ï¼ˆtrain/validation/testï¼‰")
    parser.add_argument("--outdir", type=str, default="./results/narrativeqa",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start-idx", type=int, default=0, help="å¼€å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--end-idx", type=int, default=None, help="ç»“æŸæ ·æœ¬ç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--max-tokens", type=int, default=2048, help="æ¯ä¸ªä¸Šä¸‹æ–‡å—çš„æœ€å¤§ token æ•°é‡")
    parser.add_argument("--embedding-model-path", type=str, default="BAAI/bge-m3", 
                        help="Embedding æ¨¡å‹è·¯å¾„ï¼Œç”¨äºç²¾ç¡® token è®¡ç®—ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­ï¼Œç”¨äºæ‰“ä¹±æ•°æ®é›†ï¼ˆå¯é€‰ï¼‰")
    
    # Memory Generator é…ç½®
    parser.add_argument("--memory-api-key", type=str, default="empty", help="Memory æ¨¡å‹ API Key")
    parser.add_argument("--memory-base-url", type=str, default="https://api.openai.com/v1", help="Memory æ¨¡å‹ Base URL")
    parser.add_argument("--memory-model", type=str, default="gpt-4o-mini", help="Memory æ¨¡å‹åç§°")
    
    # Research Generator é…ç½®
    parser.add_argument("--research-api-key", type=str, default="empty", help="Research æ¨¡å‹ API Key")
    parser.add_argument("--research-base-url", type=str, default="https://api.openai.com/v1", help="Research æ¨¡å‹ Base URL")
    parser.add_argument("--research-model", type=str, default="gpt-4o-mini", help="Research æ¨¡å‹åç§°")
    
    # Working Generator é…ç½®
    parser.add_argument("--working-api-key", type=str, default="empty", help="Working æ¨¡å‹ API Key")
    parser.add_argument("--working-base-url", type=str, default="https://api.openai.com/v1", help="Working æ¨¡å‹ Base URL")
    parser.add_argument("--working-model", type=str, default="gpt-4o-mini", help="Working æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("GAM æ¡†æ¶ + NarrativeQA æ•°æ®é›†æµ‹è¯•")
    print("=" * 60)
    print(f"æ•°æ®é›†ç›®å½•: {args.data_dir}")
    print(f"æ•°æ®é›†åˆ†å‰²: {args.split}")
    print(f"è¾“å‡ºç›®å½•: {args.outdir}")
    print(f"æ ·æœ¬èŒƒå›´: {args.start_idx} åˆ° {args.end_idx-1 if args.end_idx else 'å…¨éƒ¨'}")
    print(f"æœ€å¤§ token æ•°: {args.max_tokens}")
    if args.seed is not None:
        print(f"éšæœºç§å­: {args.seed}")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    all_samples = load_narrativeqa(args.data_dir, args.split)
    print(f"å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # å¦‚æœæŒ‡å®šäº† seedï¼Œæ‰“ä¹±æ•°æ®é›†
    if args.seed is not None:
        random.seed(args.seed)
        random.shuffle(all_samples)
        print(f"ä½¿ç”¨éšæœºç§å­ {args.seed} æ‰“ä¹±æ•°æ®é›†")
    
    # é‡æ–°è®¾ç½®ç»“æŸç´¢å¼•ï¼ˆåœ¨åŠ è½½æ•°æ®åï¼‰
    if args.end_idx is None:
        args.end_idx = len(all_samples)
    
    print(f"å®é™…å¤„ç†èŒƒå›´: {args.start_idx} åˆ° {args.end_idx-1} (å…± {args.end_idx - args.start_idx} ä¸ªæ ·æœ¬)")
    
    # éªŒè¯ç´¢å¼•èŒƒå›´
    if args.start_idx < 0 or args.start_idx >= len(all_samples):
        print(f"é”™è¯¯: å¼€å§‹æ ·æœ¬ç´¢å¼• {args.start_idx} è¶…å‡ºèŒƒå›´ (æ€»æ ·æœ¬æ•°: {len(all_samples)})")
        return
    
    if args.end_idx > len(all_samples):
        print(f"è­¦å‘Š: ç»“æŸæ ·æœ¬ç´¢å¼• {args.end_idx} è¶…å‡ºèŒƒå›´ï¼Œè°ƒæ•´ä¸º {len(all_samples)}")
        args.end_idx = len(all_samples)
    
    if args.start_idx >= args.end_idx:
        print(f"é”™è¯¯: å¼€å§‹ç´¢å¼• {args.start_idx} å¿…é¡»å°äºç»“æŸç´¢å¼• {args.end_idx}")
        return
    
    # ä¸²è¡Œæ‰¹é‡å¤„ç†æ ·æœ¬
    sample_indices = list(range(args.start_idx, args.end_idx))
    
    print(f"å¼€å§‹ä¸²è¡Œå¤„ç†æ ·æœ¬...")
    
    all_results = []
    for sample_idx in tqdm(sample_indices, desc="å¤„ç†æ ·æœ¬"):
        sample = all_samples[sample_idx]
        print(f"\n{'='*80}")
        print(f"å¼€å§‹å¤„ç†æ ·æœ¬ {sample_idx}/{len(all_samples)-1} (èŒƒå›´: {args.start_idx}-{args.end_idx-1})")
        print(f"{'='*80}")
        
        try:
            result = process_sample(
                sample, 
                sample_idx, 
                args.outdir,
                args.memory_api_key,
                args.memory_base_url,
                args.memory_model,
                args.research_api_key,
                args.research_base_url,
                args.research_model,
                args.working_api_key,
                args.working_base_url,
                args.working_model,
                max_tokens=args.max_tokens,
                embedding_model_path=args.embedding_model_path
            )
            print(f"[OK] æ ·æœ¬ {sample_idx} å¤„ç†å®Œæˆ")
            all_results.append(result)
        except Exception as e:
            print(f"[ERROR] æ ·æœ¬ {sample_idx} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            all_results.append({
                "sample_id": sample.get("_id", f"sample-{sample_idx}"),
                "error": str(e)
            })
    
    # ç»Ÿè®¡ç»“æœ
    f1_scores = []
    
    for result in all_results:
        if "f1" in result:
            f1_scores.append(result["f1"])
    
    # ä¿å­˜æ‰€æœ‰ç»“æœæ±‡æ€»
    if all_results:
        summary_file = os.path.join(args.outdir, f"batch_results_{args.start_idx}_{args.end_idx-1}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] æ‰¹é‡ç»“æœæ±‡æ€»å·²ä¿å­˜: {summary_file}")
        
        # è®¡ç®—å¹³å‡ F1 åˆ†æ•°
        if len(f1_scores) > 0:
            avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
            total_samples = args.end_idx - args.start_idx
            success_count = len(f1_scores) if f1_scores else len(f1_scores)
            
            # æ„å»ºç»Ÿè®¡ä¿¡æ¯
            statistics = {
                "total_samples": total_samples,
                "success_count": success_count,
                "failed_count": total_samples - success_count,
                "success_rate": success_count / total_samples if total_samples > 0 else 0.0,
                "avg_f1": avg_f1,
                "f1_scores": f1_scores,
                "start_idx": args.start_idx,
                "end_idx": args.end_idx - 1
            }
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶
            stats_file = os.path.join(args.outdir, f"batch_statistics_{args.start_idx}_{args.end_idx-1}.json")
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(statistics, f, ensure_ascii=False, indent=2)
            print(f"[OK] æ‰¹é‡æµ‹è¯•ç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"\n{'='*60}")
            print("æ‰¹é‡æµ‹è¯•ç»Ÿè®¡")
            print(f"{'='*60}")
            print(f"å¤„ç†æ ·æœ¬æ•°: {total_samples}")
            print(f"æˆåŠŸå›ç­”é—®é¢˜æ•°: {success_count}")
            print(f"å¤±è´¥é—®é¢˜æ•°: {total_samples - success_count}")
            print(f"æˆåŠŸç‡: {statistics['success_rate']:.2%}")
            print(f"å¹³å‡ F1 åˆ†æ•°: {avg_f1:.4f}")
            print(f"{'='*60}")

if __name__ == "__main__":
    main()

