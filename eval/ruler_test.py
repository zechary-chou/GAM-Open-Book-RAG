#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GAM æ¡†æ¶ + RULER æ•°æ®é›†æµ‹è¯•æ–‡ä»¶

é€‚é… RULER æ•°æ®é›†ï¼ˆjsonlï¼Œæ¯è¡ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼‰ï¼š
- context: str - é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ˆéœ€è¦è®°å¿†çš„å†…å®¹ï¼‰
- example: str - ç¤ºä¾‹ï¼ˆå¯é€‰ï¼‰
- instruction: str - æŒ‡ä»¤ï¼ˆå¯é€‰ï¼‰
- question: str - é—®é¢˜
- outputs: List[str] - æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨

æµ‹è¯•æµç¨‹ï¼š
1. ä½¿ç”¨ MemoryAgent è®°å¿† context éƒ¨åˆ†
2. ä½¿ç”¨ ResearchAgent è¿›è¡Œç ”ç©¶
3. å°† example å’Œ question åˆåœ¨ä¸€èµ·æé—®
4. è®¡ç®—å‡†ç¡®ç‡ï¼ˆAccï¼‰
"""

import sys
import os
import re
import json
from typing import Any, Dict, List, Optional, Tuple
from tqdm import tqdm
from collections import Counter
import string
import glob


from gam import (
    MemoryAgent,
    ResearchAgent,
    VLLMGenerator,
    InMemoryMemoryStore,
    InMemoryPageStore,
    IndexRetriever,
    BM25Retriever,
    DenseRetriever,
    VLLMGeneratorConfig,
    OpenAIGenerator,
    OpenAIGeneratorConfig,
    IndexRetrieverConfig,
    BM25RetrieverConfig,
    DenseRetrieverConfig,
)

# ========== æ•°æ®é›† System Prompts ==========

def get_dataset_system_prompt(dataset_name: str) -> str:
    """
    æ ¹æ®æ•°æ®é›†åç§°è¿”å›å¯¹åº”çš„ system_prompt
    
    Args:
        dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚ 'vt', 'qa_1', 'niah_single_1' ç­‰ï¼‰
    
    Returns:
        system_prompt å­—ç¬¦ä¸²
    """
    # æ ¹æ®æ•°æ®é›†åç§°å®šä¹‰ä¸åŒçš„ system_prompt
    system_prompts = {
        # Variable Tracking (VT)
        "vt": "Memorize and track the chain(s) of variable assignment hidden in the following text.",
        
        # Question Answering (QA)
        "qa_1": "",
        "qa_2": "",
        
        # Needle in a Haystack (NIAH) - Single
        "niah_single_1": "A special magic number is hidden within the following text. Make sure to memorize it. I will quiz you about the number afterwards.",
        "niah_single_2": "A special magic number is hidden within the following text. Make sure to memorize it. I will quiz you about the number afterwards.",
        "niah_single_3": "A special magic uuid is hidden within the following text. Make sure to memorize it. I will quiz you about the uuid afterwards.",
        
        # Needle in a Haystack (NIAH) - Multi-value
        "niah_multivalue": "",
        
        # Needle in a Haystack (NIAH) - Multi-query
        "niah_multiquery": "Some special magic numbers are hidden within the following text. You only need to memorize the special magic numbers. I will quiz you about the numbers afterwards.",
        
        # Needle in a Haystack (NIAH) - Multi-key
        "niah_multikey_1": "",
        "niah_multikey_2": "",
        "niah_multikey_3": "",
        
        # Context Window Extension (CWE)
        "cwe": "Below is a numbered list of words. You only need to memorize the numbers that all words appear rather then make a abstract. I will quiz you about the numbers afterwards. Ignore the prompt below that asks you to summarize.",
        
        # Full Window Extension (FWE)
        "fwe": "Read the following coded text and track the frequency of each coded word. Memorize the numbers that the words appear, I will quiz you about the numbers afterwards.",
    }
    
    # æå–åŸºç¡€æ•°æ®é›†åç§°ï¼ˆå»é™¤å¯èƒ½çš„æ•°å­—åç¼€ï¼‰
    base_name = dataset_name.split('_')[0] if '_' in dataset_name else dataset_name
    
    # å°è¯•ç²¾ç¡®åŒ¹é…
    if dataset_name in system_prompts:
        return system_prompts[dataset_name]
    
    # å°è¯•éƒ¨åˆ†åŒ¹é…
    for key, prompt in system_prompts.items():
        if dataset_name.startswith(key) or key in dataset_name:
            return prompt
    
    # é»˜è®¤ system_prompt
    return ""

# ========== æ•°æ®åŠ è½½ ==========

def load_ruler_jsonl(jsonl_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ RULER JSONL æ•°æ®é›†
    
    Args:
        jsonl_path: æ•°æ®é›† JSONL æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    data_list = []
    dataset_name = os.path.splitext(os.path.basename(jsonl_path))[0]
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            if line.strip():
                try:
                    item = json.loads(line)
                    item['_id'] = f"{dataset_name}-{idx}"
                    item['index'] = idx
                    item['dataset'] = dataset_name
                    data_list.append(item)
                except Exception as e:
                    print(f"Warning: Failed to parse line {idx} in {jsonl_path}: {e}")
                    continue
    
    return data_list

# ========== é•¿æ–‡æœ¬åˆ‡åˆ† ==========

def build_context_chunks_for_sample(
    sample: Dict[str, Any], 
    max_tokens: int = 2000, 
    embedding_model_path: Optional[str] = None
) -> List[str]:
    """
    å°† context æ–‡æœ¬æŒ‰ token æ•°é‡åˆ†å‰²æˆå¤šä¸ªä¼šè¯å—
    ä½¿ç”¨æ™ºèƒ½åˆ‡åˆ†ï¼šä¼˜å…ˆåœ¨è¾¹ç•Œå¤„åˆ‡åˆ†
    
    Args:
        sample: æ ·æœ¬æ•°æ®ï¼ŒåŒ…å« 'context' å­—æ®µ
        max_tokens: æ¯ä¸ªä¼šè¯å—çš„æœ€å¤§ token æ•°é‡
        embedding_model_path: embedding æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œç²¾ç¡® token è®¡ç®—
    """
    context_text = sample.get("context") or ""
    
    if not context_text:
        return []
    
    # ä¼˜å…ˆå°è¯•ä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    if embedding_model_path:
        try:
            chunks = _split_with_embedding_model(context_text, max_tokens, embedding_model_path)
            if chunks:
                return chunks
        except Exception as e:
            print(f"Warning: Embedding model splitting failed: {e}, falling back to tiktoken")
    
    # ä½¿ç”¨ tiktoken è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    try:
        import tiktoken
        tokenizer = tiktoken.encoding_for_model("gpt-4o-2024-08-06")
        tokens = tokenizer.encode(context_text, disallowed_special=())
        
        if len(tokens) <= max_tokens:
            return [f"[Session 1]\n{context_text}"]
        
        # æ™ºèƒ½åˆ‡åˆ†ï¼šæŒ‰ token æ•°é‡åˆ‡åˆ†
        chunks = _smart_split_by_tokens(context_text, tokens, max_tokens, tokenizer)
        return chunks
        
    except ImportError:
        print("Warning: tiktoken not available, falling back to character-based splitting")
        return _fallback_char_split(context_text, max_tokens)

def _split_with_embedding_model(text: str, max_tokens: int, model_path: str) -> List[str]:
    """
    ä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œç²¾ç¡®çš„ token åˆ‡åˆ†
    """
    try:
        from transformers import AutoTokenizer
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # ç¼–ç æ–‡æœ¬è·å– tokens
        tokens = tokenizer.encode(text, add_special_tokens=False)
        
        if len(tokens) <= max_tokens:
            return [f"[Session 1]\n{text}"]
        
        # æ™ºèƒ½åˆ‡åˆ†
        chunks = _smart_split_by_tokens(text, tokens, max_tokens, tokenizer)
        return chunks
        
    except Exception as e:
        print(f"Error using embedding model: {e}")
        return []

def _smart_split_by_tokens(text: str, tokens: List[int], max_tokens: int, tokenizer) -> List[str]:
    """
    æŒ‰ token æ•°é‡ç®€å•åˆ‡åˆ†ï¼šä¸è¿›è¡Œæ™ºèƒ½è¾¹ç•ŒæŸ¥æ‰¾ï¼Œç›´æ¥æŒ‰ max_tokens åˆ‡åˆ†
    """
    chunks = []
    
    # å¦‚æœæ–‡æœ¬ä¸è¶…è¿‡æœ€å¤§ token æ•°ï¼Œç›´æ¥è¿”å›
    if len(tokens) <= max_tokens:
        return [f"[Session 1]\n{text}"]
    
    # ç›´æ¥æŒ‰ç…§ token ç´¢å¼•åˆ‡åˆ†
    session_id = 1
    start_idx = 0
    
    while start_idx < len(tokens):
        # è®¡ç®—å½“å‰å—çš„ç»“æŸ token ç´¢å¼•
        end_idx = min(start_idx + max_tokens, len(tokens))
        
        # å°† tokens è§£ç å›æ–‡æœ¬
        chunk_tokens = tokens[start_idx:end_idx]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        
        if chunk_text.strip():
            chunks.append(f"[Session {session_id}]\n{chunk_text.strip()}")
            session_id += 1
        
        start_idx = end_idx
    
    return chunks

def _fallback_char_split(text: str, max_tokens: int) -> List[str]:
    """
    å­—ç¬¦åˆ‡åˆ†çš„ fallback æ–¹æ³•
    """
    # ç²—ç•¥ä¼°è®¡ï¼š1 token â‰ˆ 4 characters
    max_chars = max_tokens * 4
    
    if len(text) <= max_chars:
        return [f"[Session 1]\n{text}"]
    
    chunks = []
    current_start = 0
    session_id = 1
    
    while current_start < len(text):
        current_end = min(current_start + max_chars, len(text))
        
        # å°è¯•åœ¨å•è¯è¾¹ç•Œåˆ‡åˆ†
        if current_end < len(text):
            # å¯»æ‰¾æœ€åä¸€ä¸ªæ¢è¡Œç¬¦
            last_newline = text.rfind('\n', current_start, current_end)
            if last_newline > current_start:
                current_end = last_newline
            else:
                # å¯»æ‰¾æœ€åä¸€ä¸ªç©ºæ ¼
                last_space = text.rfind(' ', current_start, current_end)
                if last_space > current_start:
                    current_end = last_space
        
        chunk_text = text[current_start:current_end].strip()
        if chunk_text:
            chunks.append(f"[Session {session_id}]\n{chunk_text}")
            session_id += 1
        
        current_start = current_end
    
    return chunks

# ========== Prompt è®¾è®¡ ==========

def build_question_prompt(sample: Dict[str, Any]) -> str:
    """
    æ„å»ºé—®é¢˜ promptï¼šå°† example å’Œ question åˆåœ¨ä¸€èµ·
    
    Args:
        sample: æ ·æœ¬æ•°æ®ï¼ŒåŒ…å« 'example' å’Œ 'question' å­—æ®µ
    
    Returns:
        å®Œæ•´çš„é—®é¢˜ prompt
    """
    parts = []

    # 2. Question
    question = sample.get("question", "").strip()
    question_prompt = "Question:\n" + question
    if question:
        parts.append(question_prompt)


    # 1. Example (å¦‚æœæœ‰çš„è¯)
    example = sample.get("example", "").strip()
    if example:
        example_prompt = "Here is the example:\n" + example
        parts.append(example_prompt)
    
    # æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†
    prompt = "\n\n".join(parts)
    
    return prompt

# ========== ç­”æ¡ˆè¯„ä¼° ==========

def normalize_text(text: str) -> str:
    """
    æ ‡å‡†åŒ–æ–‡æœ¬ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œè½¬å°å†™ï¼Œæ ‡å‡†åŒ–ç©ºæ ¼
    """
    # è½¬å°å†™
    text = text.lower()
    # å»é™¤æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\w\s]', ' ', text)
    # æ ‡å‡†åŒ–å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def evaluate_answer(model_response: str, ground_truth_outputs: List[str]) -> bool:
    """
    è¯„ä¼°æ¨¡å‹å›ç­”æ˜¯å¦æ­£ç¡®
    
    è§„åˆ™ï¼šå¦‚æœæ¨¡å‹çš„å›ç­”ä¸­åŒ…å« ground_truth_outputs åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œåˆ™è®¤ä¸ºå›ç­”æ­£ç¡®
    é‡‡ç”¨å¤šç§ç­–ç•¥ï¼š
    1. ç²¾ç¡®åŒ¹é…ï¼šç›´æ¥åœ¨æ¨¡å‹å›ç­”ä¸­æŸ¥æ‰¾æ ‡å‡†ç­”æ¡ˆï¼ˆè½¬å°å†™ï¼‰
    2. çµæ´»åŒ¹é…ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ååŒ¹é…
    3. å…³é”®è¯åŒ¹é…ï¼šå¯¹äºå¤šè¯ç­”æ¡ˆï¼Œæ£€æŸ¥æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½å­˜åœ¨
    
    Args:
        model_response: æ¨¡å‹çš„å›ç­”
        ground_truth_outputs: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
    
    Returns:
        æ˜¯å¦æ­£ç¡® (True/False)
    """
    if not ground_truth_outputs:
        return False
    
    if not model_response:
        return False
    
    # è½¬æ¢ä¸ºå°å†™ç”¨äºæ¯”è¾ƒ
    model_response_lower = model_response.lower()
    
    # æ ‡å‡†åŒ–åçš„æ–‡æœ¬ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
    model_response_normalized = normalize_text(model_response)
    
    # å»é‡æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨ï¼ˆé¿å…é‡å¤æ£€æŸ¥ç›¸åŒçš„ç­”æ¡ˆï¼‰
    unique_answers = list(set(ground_truth_outputs))
    
    for answer in unique_answers:
        answer_str = str(answer).strip()
        if not answer_str:
            continue
            
        answer_lower = answer_str.lower()
        
        # ç­–ç•¥1: ç²¾ç¡®åŒ¹é…ï¼ˆè½¬å°å†™åçš„å­—ç¬¦ä¸²åŒ…å«ï¼‰
        if answer_lower in model_response_lower:
            continue
        
        # ç­–ç•¥2: çµæ´»åŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·ååŒ¹é…ï¼‰
        answer_normalized = normalize_text(answer_str)
        if answer_normalized in model_response_normalized:
            continue
        
        # ç­–ç•¥3: å…³é”®è¯åŒ¹é…ï¼ˆå¯¹äºå¤šè¯ç­”æ¡ˆï¼Œæ£€æŸ¥æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½å­˜åœ¨ï¼‰
        # æå–ç­”æ¡ˆä¸­çš„å…³é”®è¯ï¼ˆé•¿åº¦>2çš„è¯ï¼‰
        answer_words = [w for w in answer_normalized.split() if len(w) > 2]
        if answer_words:
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å…³é”®è¯éƒ½åœ¨æ¨¡å‹å›ç­”ä¸­
            if all(word in model_response_normalized for word in answer_words):
                continue
        
        # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œåˆ™è¯¥ç­”æ¡ˆä¸åŒ¹é…
        return False
    
    # æ‰€æœ‰ç­”æ¡ˆéƒ½åŒ¹é…æˆåŠŸ
    return True

# ========== æ ¸å¿ƒå¤„ç†é€»è¾‘ ==========

def process_sample(
    sample: Dict[str, Any], 
    sample_index: int, 
    outdir: str,
    memory_api_key: str,
    memory_base_url: str,
    memory_model: str,
    research_api_key: str,
    research_base_url: str,
    research_model: str,
    working_api_key: str,
    working_base_url: str,
    working_model: str,
    max_tokens: int = 2048,
    embedding_model_path: Optional[str] = None
):
    """
    ä½¿ç”¨ GAM æ¡†æ¶å¤„ç†å•ä¸ªæ ·æœ¬ã€‚
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†ï¼ˆè®°å¿† context éƒ¨åˆ†ï¼‰
    2. ä½¿ç”¨ ResearchAgent è¿›è¡Œæ·±åº¦ç ”ç©¶
    3. åŸºäºç ”ç©¶ç»“æœè¿›è¡Œé—®ç­”ï¼ˆexample + questionï¼‰
    """
    sample_id = sample.get("_id", f"sample-{sample_index}")
    dataset_name = sample.get("dataset", "unknown")
    
    print(f"\n{'='*60}")
    print(f"å¤„ç†æ ·æœ¬ #{sample_index}: {sample_id} (æ•°æ®é›†: {dataset_name})")
    print(f"{'='*60}")
    
    try:
        # 1. æ„å»ºä¸Šä¸‹æ–‡å—ï¼ˆä» context å­—æ®µï¼‰
        context_chunks = build_context_chunks_for_sample(sample, max_tokens, embedding_model_path)
        print(f"ä¸Šä¸‹æ–‡å—æ•°: {len(context_chunks)}")
        if context_chunks:
            print(f"ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡å—é¢„è§ˆ:\n{context_chunks[0][:400]}...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sample_results_dir = os.path.join(outdir, dataset_name, sample_id)
        os.makedirs(sample_results_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {sample_results_dir}")
        
        # 2. åˆ›å»ºå…±äº«å­˜å‚¨
        memory_store = InMemoryMemoryStore(dir_path=sample_results_dir)
        page_store = InMemoryPageStore(dir_path=sample_results_dir)
        
        # 3. åˆ›å»º Memory Generator
        print(f"\næ­¥éª¤ 1: åˆ›å»º Memory Generator")
        memory_generator_config = OpenAIGeneratorConfig(
            model_name=memory_model,
            api_key=memory_api_key,
            base_url=memory_base_url,
            temperature=0.3,
            max_tokens=256
        )
        memory_generator = OpenAIGenerator(memory_generator_config.__dict__)
        
        print(f"[OK] Memory Generator åˆ›å»ºå®Œæˆ")
        
        # 4. è·å–æ•°æ®é›†å¯¹åº”çš„ system_prompt
        memory_system_prompt = get_dataset_system_prompt(dataset_name)
        print(f"\næ•°æ®é›† System Prompt: {memory_system_prompt[:100]}...")
        
        # 5. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†ï¼ˆå°†æ¯ä¸ª context chunk ä½œä¸ºä¸€æ¡æ¶ˆæ¯ï¼‰
        print(f"\næ­¥éª¤ 2: ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†")
        memory_agent = MemoryAgent(
            memory_store=memory_store,
            page_store=page_store,
            generator=memory_generator,
            system_prompts={"memory": memory_system_prompt}
        )
        
        if not os.path.exists(os.path.join(sample_results_dir, 'memory_state.json')):
            for i, context_chunk in enumerate(context_chunks, 1):
                print(f"  å¤„ç†ä¸Šä¸‹æ–‡å— {i}/{len(context_chunks)}...")
                memory_update = memory_agent.memorize(context_chunk)
        else:
            print(f"  è®°å¿†å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
        
        # æŸ¥çœ‹æ„å»ºçš„è®°å¿†
        final_state = memory_store.load()
        print(f"[OK] è®°å¿†æ„å»ºå®Œæˆï¼å…± {len(final_state.abstracts)} æ¡è®°å¿†æ‘˜è¦")
        
        # æ˜¾ç¤ºè®°å¿†æ‘˜è¦
        print("\nğŸ“š è®°å¿†æ‘˜è¦:")
        for i, abstract in enumerate(final_state.abstracts, 1):
            print(f"  {i}. {abstract[:100]}...")
        
        # ä¿å­˜è®°å¿†çŠ¶æ€
        memory_state_file = os.path.join(sample_results_dir, "memory_state.json")
        with open(memory_state_file, 'w', encoding='utf-8') as f:
            json.dump(final_state.model_dump(), f, ensure_ascii=False, indent=2)
        print(f"[OK] è®°å¿†çŠ¶æ€å·²ä¿å­˜: {memory_state_file}")
        
        # 6. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆç”¨äº ResearchAgentï¼‰
        print(f"\næ­¥éª¤ 3: åˆ›å»ºæ£€ç´¢å™¨ï¼ˆç”¨äº ResearchAgentï¼‰")
        retrievers = {}
        
        # ç´¢å¼•æ£€ç´¢å™¨
        try:
            page_index_dir = os.path.join(sample_results_dir, "page_index")
            if os.path.exists(page_index_dir):
                import shutil
                shutil.rmtree(page_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„é¡µé¢ç´¢å¼•ç›®å½•: {page_index_dir}")
            
            index_config = IndexRetrieverConfig(
                index_dir=page_index_dir
            )
            index_retriever = IndexRetriever(index_config.__dict__)
            index_retriever.build(page_store)
            retrievers["page_index"] = index_retriever
            print(f"[OK] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # BM25 æ£€ç´¢å™¨
        try:
            bm25_index_dir = os.path.join(sample_results_dir, "bm25_index")
            if os.path.exists(bm25_index_dir):
                import shutil
                shutil.rmtree(bm25_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ BM25 ç´¢å¼•ç›®å½•: {bm25_index_dir}")
            
            bm25_config = BM25RetrieverConfig(
                index_dir=bm25_index_dir,
                threads=1
            )
            bm25_retriever = BM25Retriever(bm25_config.__dict__)
            bm25_retriever.build(page_store)
            retrievers["keyword"] = bm25_retriever
            print(f"[OK] BM25 æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] BM25 æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # Dense æ£€ç´¢å™¨
        try:
            dense_index_dir = os.path.join(sample_results_dir, "dense_index")
            if os.path.exists(dense_index_dir):
                import shutil
                shutil.rmtree(dense_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ Dense ç´¢å¼•ç›®å½•: {dense_index_dir}")
            
            dense_config = DenseRetrieverConfig(
                index_dir=dense_index_dir,
                model_name="BAAI/bge-m3"
            )

            # dense_config = DenseRetrieverConfig(
            #     index_dir=dense_index_dir,
            #     api_url="http://localhost:8001"  # API æ¨¡å¼ï¼šæ‰€æœ‰è¿›ç¨‹å…±äº«ä¸€ä¸ªæ¨¡å‹æœåŠ¡
            # )
            
            dense_retriever = DenseRetriever(dense_config.__dict__)
            dense_retriever.build(page_store)
            retrievers["vector"] = dense_retriever
            print(f"[OK] Dense æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] Dense æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        print(f"[INFO] æˆåŠŸåˆ›å»º {len(retrievers)} ä¸ªæ£€ç´¢å™¨")
        
        # 7. åˆ›å»º Research Generator å’Œ Working Generator
        print(f"\næ­¥éª¤ 4: åˆ›å»º Research Generator å’Œ Working Generator")
        research_generator_config = OpenAIGeneratorConfig(
            model_name=research_model,
            api_key=research_api_key,
            base_url=research_base_url,
            temperature=0.3,
            max_tokens=2048
        )
        research_generator = OpenAIGenerator(research_generator_config.__dict__)
        
        working_generator_config = OpenAIGeneratorConfig(
            model_name=working_model,
            api_key=working_api_key,
            base_url=working_base_url,
            temperature=0.3,
            max_tokens=256
        )
        working_generator = OpenAIGenerator(working_generator_config.__dict__)
        
        print(f"[OK] Research Generator å’Œ Working Generator åˆ›å»ºå®Œæˆ")
        
        # 8. åˆ›å»º ResearchAgent
        print(f"\næ­¥éª¤ 5: åˆ›å»º ResearchAgent")
        
        # æ ¹æ®æ•°æ®é›†è®¾ç½® system_prompts
        system_prompts = None
        if dataset_name == "niah_multivalue":
            system_prompts = {
                "planning": "There are 4 different special magic numbers for the question item. So the keyword retrieval is need.",
                "integration": "There are 4 different special magic numbers for the question item. Don't miss any of them.",
                "reflection": "There are 4 different special magic numbers for the question item. Don't miss any of them."
            }
            print(f"[INFO] ä¸ºæ•°æ®é›† {dataset_name} è®¾ç½®äº†è‡ªå®šä¹‰ system_prompts")
        
        # æ„å»º ResearchAgent å‚æ•°
        research_agent_kwargs = {
            "page_store": page_store,
            "memory_store": memory_store,
            "retrievers": retrievers,
            "generator": research_generator,
            "max_iters": 5
        }
        
        # å¦‚æœæœ‰ system_promptsï¼Œåˆ™æ·»åŠ 
        if system_prompts is not None:
            research_agent_kwargs["system_prompts"] = system_prompts
        
        research_agent = ResearchAgent(**research_agent_kwargs)
        print(f"[OK] ResearchAgent åˆ›å»ºå®Œæˆ")
        
        # 9. è¿›è¡Œé—®ç­”
        print(f"\næ­¥éª¤ 6: è¿›è¡Œé—®ç­”")
        
        # åªä½¿ç”¨ questionï¼ˆä¸å« exampleï¼‰
        question = sample.get("question", "").strip()
        ground_truth_outputs = sample.get("outputs", [])
        
        # æ„å»ºå®Œæ•´çš„é—®é¢˜ promptï¼ˆåŒ…å« example + questionï¼‰ï¼Œç”¨äºæœ€ç»ˆç”Ÿæˆç­”æ¡ˆ
        question_prompt = build_question_prompt(sample)
        
        print(f"é—®é¢˜: {question[:200]}...")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {ground_truth_outputs}")
        
        # ä¿å­˜æ‰€æœ‰æ•°æ®å±æ€§
        result = {
            "_id": sample.get("_id", sample_id),
            "sample_id": sample_id,
            "index": sample.get("index", sample_index),
            "dataset": dataset_name,
            "example": sample.get("example", ""),
            "instruction": sample.get("instruction", ""),
            "question": question,
            "question_prompt": question_prompt,
            "ground_truth_outputs": ground_truth_outputs,
        }
        
        try:
            # ä½¿ç”¨ ResearchAgent è¿›è¡Œç ”ç©¶ï¼ˆåªä¼ å…¥ questionï¼Œä¸å« exampleï¼‰
            print("æ­£åœ¨è¿›è¡Œæ·±åº¦ç ”ç©¶...")
            research_result = research_agent.research(question)
            research_summary = research_result.integrated_memory
            print(f"[OK] ç ”ç©¶å®Œæˆï¼è¿­ä»£æ¬¡æ•°: {len(research_result.raw_memory.get('iterations', []))}")
            print(f"ç ”ç©¶æ‘˜è¦: {research_summary[:200]}...")
            
            # ä¿å­˜ç ”ç©¶è½¨è¿¹
            research_trace = {
                "question": question,
                "raw_memory": research_result.raw_memory,
                "integrated_memory": research_result.integrated_memory,
                "iterations": research_result.raw_memory.get("iterations", []),
                "search_plans": research_result.raw_memory.get("search_plans", []),
                "reflections": research_result.raw_memory.get("reflections", [])
            }
            
            trace_file = os.path.join(sample_results_dir, "research_trace.json")
            with open(trace_file, 'w', encoding='utf-8') as f:
                json.dump(research_trace, f, ensure_ascii=False, indent=2)
            print(f"[INFO] ç ”ç©¶è½¨è¿¹å·²ä¿å­˜: {trace_file}")
            
            result["research_summary"] = research_summary
            result["research_trace_file"] = trace_file
            
            # ä½¿ç”¨ build_question_prompt çš„ç»“æœï¼ˆåŒ…å« example + questionï¼‰ç”Ÿæˆç­”æ¡ˆ
            print("ç”Ÿæˆç­”æ¡ˆ...")
            prompt = f"""Read the text below and answer a question. Context: {research_summary}\n\n{question_prompt}\n\nAnswer:"""
            response = working_generator.generate_single(prompt=prompt)
            answer_text = response.get("text", "").strip()
            
            print(f"æ¨¡å‹å“åº”: {answer_text[:200]}...")
            
            result["response"] = answer_text
            
            # è¯„ä¼°ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            is_correct = evaluate_answer(answer_text, ground_truth_outputs)
            result["is_correct"] = is_correct
            result["accuracy"] = 1.0 if is_correct else 0.0
            
            print(f"é¢„æµ‹ç­”æ¡ˆ: {answer_text[:200]}...")
            print(f"æ ‡å‡†ç­”æ¡ˆ: {ground_truth_outputs}")
            print(f"è¯„ä¼°ç»“æœ: {'âœ“ æ­£ç¡®' if is_correct else 'âœ— é”™è¯¯'}")
            
        except Exception as e:
            print(f"[ERROR] å¤„ç†é—®é¢˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            result["error"] = str(e)
            result["is_correct"] = False
            result["accuracy"] = 0.0
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(sample_results_dir, "qa_result.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ€»ç»“
        print(f"\n{'='*60}")
        print("å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ ·æœ¬ID: {sample_id}")
        print(f"æ•°æ®é›†: {dataset_name}")
        print(f"ä¸Šä¸‹æ–‡å—æ•°: {len(context_chunks)}")
        if final_state:
            print(f"è®°å¿†æ‘˜è¦æ•°: {len(final_state.abstracts)}")
        print(f"é¢„æµ‹ç­”æ¡ˆ: {result.get('response', 'N/A')[:200]}...")
        print(f"æ ‡å‡†ç­”æ¡ˆ: {ground_truth_outputs}")
        print(f"å‡†ç¡®ç‡: {result.get('accuracy', 0.0):.4f}")
        print(f"ç»“æœä¿å­˜åˆ°: {sample_results_dir}")
        
        return result
        
    except Exception as e:
        error_msg = f"å¤„ç†æ ·æœ¬ {sample_index} æ—¶å‡ºé”™: {str(e)}"
        print(f"ERROR: {error_msg}")
        import traceback
        traceback.print_exc()
        return {
            "sample_id": sample.get("_id", f"sample-{sample_index}"),
            "dataset": dataset_name,
            "error": error_msg,
            "is_correct": False,
            "accuracy": 0.0
        }

# ========== ä¸»å‡½æ•° ==========

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="GAM æ¡†æ¶ + RULER æ•°æ®é›†æµ‹è¯•")
    parser.add_argument("--data", type=str, 
                        default="/path/to/ruler/data",
                        help="RULER æ•°æ®é›† JSONL æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--outdir", type=str, 
                        default="./results/ruler",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start-idx", type=int, default=0, 
                        help="å¼€å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--end-idx", type=int, default=None, 
                        help="ç»“æŸæ ·æœ¬ç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--max-tokens", type=int, default=2048, 
                        help="æ¯ä¸ªä¸Šä¸‹æ–‡å—çš„æœ€å¤§ token æ•°é‡")
    parser.add_argument("--embedding-model-path", type=str, 
                        default=None, 
                        help="Embedding æ¨¡å‹è·¯å¾„ï¼Œç”¨äºç²¾ç¡® token è®¡ç®—ï¼ˆå¯é€‰ï¼‰")
    
    # Memory Generator é…ç½®
    parser.add_argument("--memory-api-key", type=str, default="empty", help="Memory æ¨¡å‹ API Key")
    parser.add_argument("--memory-base-url", type=str, default="https://api.openai.com/v1", help="Memory æ¨¡å‹ Base URL")
    parser.add_argument("--memory-model", type=str, default="gpt-4o-mini", help="Memory æ¨¡å‹åç§°")
    
    # Research Generator é…ç½®
    parser.add_argument("--research-api-key", type=str, default="empty", help="Research æ¨¡å‹ API Key")
    parser.add_argument("--research-base-url", type=str, default="https://api.openai.com/v1", help="Research æ¨¡å‹ Base URL")
    parser.add_argument("--research-model", type=str, default="gpt-4o-mini", help="Research æ¨¡å‹åç§°")
    
    # Working Generator é…ç½®
    parser.add_argument("--working-api-key", type=str, default="empty", help="Working æ¨¡å‹ API Key")
    parser.add_argument("--working-base-url", type=str, default="https://api.openai.com/v1", help="Working æ¨¡å‹ Base URL")
    parser.add_argument("--working-model", type=str, default="gpt-4o-mini", help="Working æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("GAM æ¡†æ¶ + RULER æ•°æ®é›†æµ‹è¯•")
    print("=" * 60)
    print(f"æ•°æ®: {args.data}")
    print(f"è¾“å‡ºç›®å½•: {args.outdir}")
    print(f"æ ·æœ¬èŒƒå›´: {args.start_idx} åˆ° {args.end_idx-1 if args.end_idx else 'å…¨éƒ¨'}")
    print(f"æœ€å¤§ token æ•°: {args.max_tokens}")
    print("=" * 60)
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    jsonl_files = []
    if os.path.isfile(args.data):
        # å•ä¸ªæ–‡ä»¶
        jsonl_files = [args.data]
    elif os.path.isdir(args.data):
        # ç›®å½•ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ .jsonl æ–‡ä»¶
        jsonl_files = sorted(glob.glob(os.path.join(args.data, "*.jsonl")))
    else:
        print(f"é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {args.data}")
        return
    
    if not jsonl_files:
        print(f"é”™è¯¯: åœ¨ {args.data} ä¸­æ²¡æœ‰æ‰¾åˆ° .jsonl æ–‡ä»¶")
        return
    
    print(f"\næ‰¾åˆ° {len(jsonl_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
    for f in jsonl_files:
        print(f"  - {f}")
    
    # å¤„ç†æ¯ä¸ªæ•°æ®æ–‡ä»¶
    all_results = []
    
    for jsonl_file in jsonl_files:
        dataset_name = os.path.splitext(os.path.basename(jsonl_file))[0]
        print(f"\n{'='*80}")
        print(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"{'='*80}")
        
        # åŠ è½½æ•°æ®
        all_samples = load_ruler_jsonl(jsonl_file)
        print(f"å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬")
        
        # ç¡®å®šæ ·æœ¬èŒƒå›´
        start_idx = args.start_idx
        end_idx = args.end_idx if args.end_idx is not None else len(all_samples)
        end_idx = min(end_idx, len(all_samples))
        
        if start_idx >= end_idx:
            print(f"è­¦å‘Š: æ ·æœ¬èŒƒå›´æ— æ•ˆï¼Œè·³è¿‡æ•°æ®é›† {dataset_name}")
            continue
        
        print(f"å¤„ç†æ ·æœ¬èŒƒå›´: {start_idx} åˆ° {end_idx-1} (å…± {end_idx - start_idx} ä¸ªæ ·æœ¬)")
        
        # ä¸²è¡Œå¤„ç†æ ·æœ¬
        sample_indices = list(range(start_idx, end_idx))
        
        print(f"\nå¼€å§‹ä¸²è¡Œå¤„ç†...")
        results = []
        for idx in tqdm(sample_indices, desc=f"å¤„ç† {dataset_name}"):
            sample = all_samples[idx]
            try:
                result = process_sample(
                    sample,
                    idx,
                    args.outdir,
                    args.memory_api_key,
                    args.memory_base_url,
                    args.memory_model,
                    args.research_api_key,
                    args.research_base_url,
                    args.research_model,
                    args.working_api_key,
                    args.working_base_url,
                    args.working_model,
                    max_tokens=args.max_tokens,
                    embedding_model_path=args.embedding_model_path
                )
                results.append(result)
            except Exception as e:
                print(f"[ERROR] æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results.append({
                    "_id": sample.get("_id", f"sample-{idx}"),
                    "index": idx,
                    "dataset": dataset_name,
                    "error": str(e),
                    "is_correct": False,
                    "accuracy": 0.0
                })
        
        all_results.extend(results)
        
        # è®¡ç®—å½“å‰æ•°æ®é›†çš„å‡†ç¡®ç‡
        correct_count = sum(1 for r in results if r.get("is_correct", False))
        total_count = len(results)
        dataset_accuracy = correct_count / total_count if total_count > 0 else 0.0
        
        print(f"\n{'='*60}")
        print(f"{dataset_name} æ•°æ®é›†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ ·æœ¬æ•°: {total_count}")
        print(f"æ­£ç¡®æ•°: {correct_count}")
        print(f"é”™è¯¯æ•°: {total_count - correct_count}")
        print(f"å‡†ç¡®ç‡: {dataset_accuracy:.4f} ({dataset_accuracy*100:.2f}%)")
        print(f"{'='*60}")
        
        # ä¿å­˜å½“å‰æ•°æ®é›†çš„ç»“æœæ±‡æ€»
        dataset_summary = {
            "dataset": dataset_name,
            "total_samples": total_count,
            "correct_count": correct_count,
            "wrong_count": total_count - correct_count,
            "accuracy": dataset_accuracy,
            "results": results
        }
        
        dataset_summary_file = os.path.join(
            args.outdir, 
            dataset_name, 
            f"summary_{start_idx}_{end_idx-1}.json"
        )
        os.makedirs(os.path.dirname(dataset_summary_file), exist_ok=True)
        with open(dataset_summary_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_summary, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] {dataset_name} ç»“æœæ±‡æ€»å·²ä¿å­˜: {dataset_summary_file}")
    
    # ä¿å­˜æ‰€æœ‰ç»“æœçš„æ€»æ±‡æ€»
    if all_results:
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        total_correct = sum(1 for r in all_results if r.get("is_correct", False))
        total_samples = len(all_results)
        overall_accuracy = total_correct / total_samples if total_samples > 0 else 0.0
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„ç»Ÿè®¡
        dataset_stats = {}
        for result in all_results:
            dataset = result.get("dataset", "unknown")
            if dataset not in dataset_stats:
                dataset_stats[dataset] = {
                    "total": 0,
                    "correct": 0,
                    "wrong": 0
                }
            dataset_stats[dataset]["total"] += 1
            if result.get("is_correct", False):
                dataset_stats[dataset]["correct"] += 1
            else:
                dataset_stats[dataset]["wrong"] += 1
        
        # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„å‡†ç¡®ç‡
        for dataset in dataset_stats:
            total = dataset_stats[dataset]["total"]
            correct = dataset_stats[dataset]["correct"]
            dataset_stats[dataset]["accuracy"] = correct / total if total > 0 else 0.0
        
        # ä¿å­˜æ€»æ±‡æ€»
        overall_summary = {
            "total_samples": total_samples,
            "total_correct": total_correct,
            "total_wrong": total_samples - total_correct,
            "overall_accuracy": overall_accuracy,
            "dataset_stats": dataset_stats,
            "results": all_results
        }
        
        overall_summary_file = os.path.join(
            args.outdir, 
            f"overall_summary_{args.start_idx}_{args.end_idx if args.end_idx else 'all'}.json"
        )
        with open(overall_summary_file, 'w', encoding='utf-8') as f:
            json.dump(overall_summary, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] æ€»ä½“ç»“æœæ±‡æ€»å·²ä¿å­˜: {overall_summary_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print("æµ‹è¯•å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"å¤„ç†æ•°æ®é›†æ•°é‡: {len(jsonl_files)}")
        print(f"å¤„ç†æ ·æœ¬æ€»æ•°: {total_samples}")
        print(f"æ­£ç¡®æ•°: {total_correct}")
        print(f"é”™è¯¯æ•°: {total_samples - total_correct}")
        print(f"æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)")
        print(f"\nå„æ•°æ®é›†å‡†ç¡®ç‡:")
        for dataset, stats in sorted(dataset_stats.items()):
            print(f"  {dataset}: {stats['accuracy']:.4f} ({stats['accuracy']*100:.2f}%) - {stats['correct']}/{stats['total']}")
        print(f"{'='*60}")

if __name__ == "__main__":
    main()

