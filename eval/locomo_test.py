#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GAM æ¡†æ¶ + LoCoMo æ•°æ®é›†æµ‹è¯•æ–‡ä»¶

ç»“åˆ locomoqa_v3.py çš„æ•°æ®å¤„ç†é€»è¾‘å’Œ GAM æ¡†æ¶ï¼Œæµ‹è¯•åœ¨å¤šè½®å¯¹è¯æ•°æ®ä¸Šçš„æ•ˆæœã€‚
"""

import sys
import os
import re
import json
import math
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict, Counter
from tqdm import tqdm


from gam import (
    MemoryAgent,
    ResearchAgent,
    InMemoryMemoryStore,
    InMemoryPageStore,
    IndexRetriever,
    BM25Retriever,
    DenseRetriever,
    VLLMGenerator,
    VLLMGeneratorConfig,
    OpenAIGenerator,
    OpenAIGeneratorConfig,
    IndexRetrieverConfig,
    BM25RetrieverConfig,
    DenseRetrieverConfig,
)

# ========== æ•°æ®åŠ è½½ï¼šå€Ÿé‰´è‡ª locomoqa_v3.py ==========

def load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def load_locomo(json_path: str) -> List[Dict[str, Any]]:
    """Load LoCoMo JSON and return the list of samples."""
    data = load_json(json_path)
    if isinstance(data, dict) and "samples" in data:
        return data["samples"]
    if isinstance(data, list):
        return data
    raise ValueError("Unrecognized LoCoMo JSON shape. Expect a list or {'samples': [...]}.")

def extract_sessions(conv_obj: Dict[str, Any]) -> List[Tuple[int, str, List[Dict[str, Any]], Optional[str]]]:
    """
    Extract sessions as (idx, timestamp, turns, optional_session_summary).
    """
    sessions: List[Tuple[int, str, List[Dict[str, Any]], Optional[str]]] = []
    for k, v in conv_obj.items():
        m = re.match(r'^session_(\d+)$', k)
        if not (m and isinstance(v, list)):
            continue
        original_idx = int(m.group(1))
        idx = original_idx - 1
        ts = conv_obj.get(f"session_{original_idx}_date_time", "")
        ssum = conv_obj.get(f"session_{original_idx}_summary", None)
        sessions.append((idx, ts, v, ssum if isinstance(ssum, str) and ssum.strip() else None))
    sessions.sort(key=lambda x: x[0])
    return sessions

def session_to_text(idx: int, ts: str, turns: List[Dict[str, Any]], session_summary: Optional[str]) -> str:
    # å°†æ—¶é—´ä¿¡æ¯æ”¾åœ¨æœ€å‰é¢ï¼Œä½¿ç”¨æ›´çªå‡ºçš„æ ¼å¼
    lines = [f"=== SESSION {idx} - Dialogue Time(available to answer questions): {ts} ==="]
    lines.append("")  # ç©ºè¡Œåˆ†éš”
    
    for turn in turns:
        speaker = turn.get("speaker", "Unknown")
        dia_id  = turn.get("dia_id", "")
        text    = turn.get("text", "")
        lines.append(f"{speaker} ({dia_id}): {text}")
    
    if session_summary:
        lines.append("")
        lines.append(f"Session {idx} summary: {session_summary}")
    
    return "\n".join(lines).strip()

def build_session_chunks_for_sample(sample: Dict[str, Any]) -> List[str]:
    """Build session chunks from a sample."""
    conv = sample.get("conversation", {})
    sessions = extract_sessions(conv)
    chunks: List[str] = []
    for idx, ts, turns, ssum in sessions:
        chunks.append(session_to_text(idx, ts, turns, ssum))
    return chunks

def collect_qa_items_for_sample(sample: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Collect QA items from a sample."""
    qas: List[Dict[str, Any]] = []
    sid = sample.get("sample_id", None)
    for q in sample.get("qa", []):
        qas.append({
            "sample_id": sid,
            "question": q.get("question"),
            "answer": q.get("answer"),
            "category": q.get("category"),
            "evidence": q.get("evidence"),
        })
    return qas

# ========== Prompt è®¾è®¡ï¼šå®Œå…¨å€Ÿé‰´è‡ª locomoqa_v3.py ==========

def safe_json_extract(candidate: Any) -> Optional[Dict[str, Any]]:
    """å°½é‡æŠŠæ¨¡å‹è¾“å‡ºï¼ˆstring/dictï¼‰è§£ææˆ dictï¼Œå¤±è´¥è¿”å› Noneã€‚"""
    if isinstance(candidate, dict):
        return candidate
    if not isinstance(candidate, str):
        return None
    s = candidate.strip()
    l = s.find('{')
    r = s.rfind('}')
    if l == -1 or r == -1 or r <= l:
        return None
    try:
        return json.loads(s[l:r+1])
    except Exception:
        return None

def make_summary_prompt(summary: str, question: str) -> str:
    return f"""\
Based on the summary below, write an answer in the form of **a short phrase** for the following question, not a sentence. Answer with exact words from the context whenever possible.
For questions that require answering a date or time, strictly follow the format \"15 July 2023\" and provide a specific date whenever possible. For example, if you need to answer \"last year,\" give the specific year of last year rather than just saying \"last year.\" Only provide one year, date, or time, without any extra responses.
If the question is about the duration, answer in the form of several years, months, or days.

QUESTION:
{question}

SUMMARY:
{summary}

Short answer:
"""

def make_summary_prompt_category3(summary: str, question: str) -> str:
    return f"""\
Based on the summary below, write an answer in the form of **a short phrase** for the following question, not a sentence.
The question may need you to analyze and infer the answer from the summary.
    
QUESTION:
{question}

SUMMARY:
{summary}

Short answer:
"""

def answer_with_summary(category: Optional[int], summary: str, question: str, generator) -> str:
    """æ ¹æ®categoryé€‰æ‹©ä¸åŒçš„prompt"""
    if category == 3:
        prompt = make_summary_prompt_category3(summary, question)
    else:
        prompt = make_summary_prompt(summary, question)
    raw = generator.generate_single(prompt=prompt)
    return raw.get("text", "").strip()

# ========== æŒ‡æ ‡è®¡ç®—ï¼šå€Ÿé‰´è‡ª eval_metric_locomo.py ==========

def normalize_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.lower().strip()
    s = re.sub(r"[^\w\s]", " ", s)   # remove punctuation
    s = re.sub(r"\s+", " ", s).strip()
    s = re.sub(r"(^|\s)(a|an|the)(\s|$)", " ", s)  # drop english articles
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokens(s: str):
    s = normalize_text(s)
    return s.split() if s else []

def f1_score(pred: str, gold: str) -> float:
    gtoks = tokens(gold)
    ptoks = tokens(pred)
    if not gtoks and not ptoks:
        return 1.0
    if not gtoks or not ptoks:
        return 0.0
    gcount = Counter(gtoks)
    pcount = Counter(ptoks)
    overlap = sum(min(pcount[t], gcount[t]) for t in pcount)
    if overlap == 0:
        return 0.0
    precision = overlap / len(ptoks)
    recall = overlap / len(gtoks)
    if precision + recall == 0:
        return 0.0
    return 2 * precision * recall / (precision + recall)

def bleu1_score(pred: str, gold: str) -> float:
    gtoks = tokens(gold)
    ptoks = tokens(pred)
    if len(ptoks) == 0:
        return 0.0
    gcount = Counter(gtoks)
    pcount = Counter(ptoks)
    clipped = sum(min(pcount[t], gcount[t]) for t in pcount)
    precision = clipped / len(ptoks) if ptoks else 0.0
    if ptoks and gtoks:
        bp = 1.0 if len(ptoks) >= len(gtoks) else math.exp(1 - len(gtoks)/len(ptoks))
    else:
        bp = 0.0
    return bp * precision

def compute_metrics_by_category(items, pred_key: str = "summary_answer", pred_field: str = "answer"):
    agg = defaultdict(list)
    rows = []
    for idx, ex in enumerate(items, 1):
        cat = ex.get("category", "NA")
        gold = ex.get("gold_answer", "")
        pred = ""
        val = ex.get(pred_key, "")
        if isinstance(val, dict):
            pred = val.get(pred_field, "")
        else:
            pred = val
        f1 = f1_score(pred, gold)
        b1 = bleu1_score(pred, gold)
        agg[cat].append((f1, b1))
        rows.append({
            "q_idx": idx,
            "category": cat,
            "gold_answer": str(gold),
            "prediction": str(pred),
            "F1": f1,
            "BLEU1": b1
        })
    summary = []
    for cat in sorted(agg.keys(), key=lambda x: str(x)):
        scores = agg[cat]
        if scores:
            f1_avg = sum(s[0] for s in scores)/len(scores)
            b1_avg = sum(s[1] for s in scores)/len(scores)
            summary.append({"category": cat, "count": len(scores), "F1_avg": f1_avg, "BLEU1_avg": b1_avg})
    return summary, rows

# ========== æ ¸å¿ƒå¤„ç†é€»è¾‘ ==========

def process_sample(
    sample: Dict[str, Any], 
    sample_index: int, 
    outdir: str,
    memory_api_key: str,
    memory_base_url: str,
    memory_model: str,
    research_api_key: str,
    research_base_url: str,
    research_model: str,
    working_api_key: str,
    working_base_url: str,
    working_model: str
):
    """
    ä½¿ç”¨ GAM æ¡†æ¶å¤„ç†å•ä¸ªæ ·æœ¬ã€‚
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†
    2. ä½¿ç”¨ ResearchAgent è¿›è¡Œæ·±åº¦ç ”ç©¶
    3. åŸºäºç ”ç©¶ç»“æœè¿›è¡Œé—®ç­”
    """
    sample_id = sample.get("sample_id", f"conv-{sample_index}")
    
    print(f"\n{'='*60}")
    print(f"å¤„ç†æ ·æœ¬ #{sample_index}: {sample_id}")
    print(f"{'='*60}")
    
    try:
        # 1. æ„å»ºä¼šè¯å—
        session_chunks = build_session_chunks_for_sample(sample)
        print(f"ä¼šè¯æ•°: {len(session_chunks)}")
        if session_chunks:
            print(f"ç¬¬ä¸€ä¸ªä¼šè¯é¢„è§ˆ:\n{session_chunks[0][:400]}...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sample_results_dir = os.path.join(outdir, sample_id)
        os.makedirs(sample_results_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {sample_results_dir}")
        
        # 2. åˆ›å»ºå…±äº«å­˜å‚¨
        memory_store = InMemoryMemoryStore(dir_path=sample_results_dir)
        page_store = InMemoryPageStore(dir_path=sample_results_dir)
        
        # 3. åˆ›å»º Memory Generator
        print(f"\næ­¥éª¤ 1: åˆ›å»º Memory Generator")
        memory_generator_config = OpenAIGeneratorConfig(
            model_name=memory_model,
            api_key=memory_api_key,
            base_url=memory_base_url,
            temperature=0.3,
            max_tokens=256
        )
        memory_generator = OpenAIGenerator(memory_generator_config.__dict__)
        
        print(f"[OK] Memory Generator åˆ›å»ºå®Œæˆ")
        
        # 4. ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†ï¼ˆå°†æ¯ä¸ª session ä½œä¸ºä¸€æ¡æ¶ˆæ¯ï¼‰
        print(f"\næ­¥éª¤ 2: ä½¿ç”¨ MemoryAgent æ„å»ºè®°å¿†")
        memory_agent = MemoryAgent(
            memory_store=memory_store,
            page_store=page_store,
            generator=memory_generator
        )
        
        if not os.path.exists(os.path.join(sample_results_dir, 'memory_state.json')):
            for i, session_chunk in enumerate(session_chunks, 1):
                print(f"  å¤„ç†ä¼šè¯ {i}/{len(session_chunks)}...")
                memory_update = memory_agent.memorize(session_chunk)
        
        # æŸ¥çœ‹æ„å»ºçš„è®°å¿†
        final_state = memory_store.load()
        print(f"[OK] è®°å¿†æ„å»ºå®Œæˆï¼å…± {len(final_state.abstracts)} æ¡è®°å¿†æ‘˜è¦")
        
        # æ˜¾ç¤ºè®°å¿†æ‘˜è¦
        print("\nğŸ“š è®°å¿†æ‘˜è¦:")
        for i, abstract in enumerate(final_state.abstracts, 1):
            print(f"  {i}. {abstract[:100]}...")
        
        # ä¿å­˜è®°å¿†çŠ¶æ€
        memory_state_file = os.path.join(sample_results_dir, "memory_state.json")
        with open(memory_state_file, 'w', encoding='utf-8') as f:
            json.dump(final_state.model_dump(), f, ensure_ascii=False, indent=2)
        print(f"[OK] è®°å¿†çŠ¶æ€å·²ä¿å­˜: {memory_state_file}")
        
        # 5. åˆ›å»ºæ£€ç´¢å™¨
        print(f"\næ­¥éª¤ 3: åˆ›å»ºæ£€ç´¢å™¨")
        retrievers = {}
        
        # ç´¢å¼•æ£€ç´¢å™¨
        try:
            page_index_dir = os.path.join(sample_results_dir, "page_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(page_index_dir):
                import shutil
                shutil.rmtree(page_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„é¡µé¢ç´¢å¼•ç›®å½•: {page_index_dir}")
            
            index_config = IndexRetrieverConfig(
                index_dir=page_index_dir
            )
            index_retriever = IndexRetriever(index_config.__dict__)
            index_retriever.build(page_store)
            retrievers["page_index"] = index_retriever
            print(f"[OK] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] ç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # BM25 æ£€ç´¢å™¨
        try:
            bm25_index_dir = os.path.join(sample_results_dir, "bm25_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(bm25_index_dir):
                import shutil
                shutil.rmtree(bm25_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ BM25 ç´¢å¼•ç›®å½•: {bm25_index_dir}")
            
            bm25_config = BM25RetrieverConfig(
                index_dir=bm25_index_dir,
                threads=1
            )
            bm25_retriever = BM25Retriever(bm25_config.__dict__)
            bm25_retriever.build(page_store)
            retrievers["keyword"] = bm25_retriever
            print(f"[OK] BM25 æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] BM25 æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        # Dense æ£€ç´¢å™¨
        try:
            dense_index_dir = os.path.join(sample_results_dir, "dense_index")
            # å¦‚æœç´¢å¼•ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒï¼ˆé¿å… "Directory not empty" é”™è¯¯ï¼‰
            if os.path.exists(dense_index_dir):
                import shutil
                shutil.rmtree(dense_index_dir)
                print(f"[INFO] æ¸…ç†å·²å­˜åœ¨çš„ Dense ç´¢å¼•ç›®å½•: {dense_index_dir}")

            dense_config = DenseRetrieverConfig(
                index_dir=dense_index_dir,
                model_name="BAAI/bge-m3"
            )

            # dense_config = DenseRetrieverConfig(
            #     index_dir=dense_index_dir,
            #     api_url="http://localhost:8001"  # API æ¨¡å¼ï¼šæ‰€æœ‰è¿›ç¨‹å…±äº«ä¸€ä¸ªæ¨¡å‹æœåŠ¡
            # )

            dense_retriever = DenseRetriever(dense_config.__dict__)
            dense_retriever.build(page_store)
            retrievers["vector"] = dense_retriever
            print(f"[OK] Dense æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"[WARN] Dense æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        
        print(f"[INFO] æˆåŠŸåˆ›å»º {len(retrievers)} ä¸ªæ£€ç´¢å™¨")
        
        print(f"\næ­¥éª¤ 4: åˆ›å»º Research Generator å’Œ Working Generator")
        research_generator_config = OpenAIGeneratorConfig(
            model_name=research_model,
            api_key=research_api_key,
            base_url=research_base_url,
            temperature=0.3,
            max_tokens=2048
        )
        research_generator = OpenAIGenerator(research_generator_config.__dict__)
        
        working_generator_config = OpenAIGeneratorConfig(
            model_name=working_model,
            api_key=working_api_key,
            base_url=working_base_url,
            temperature=0.3,
            max_tokens=256
        )
        working_generator = OpenAIGenerator(working_generator_config.__dict__)
        
        print(f"[OK] Research Generator å’Œ Working Generator åˆ›å»ºå®Œæˆ")


        # 6. åˆ›å»º ResearchAgent
        print(f"\næ­¥éª¤ 5: åˆ›å»º ResearchAgent")
        research_agent = ResearchAgent(
            page_store=page_store,
            memory_store=memory_store,
            retrievers=retrievers,
            generator=research_generator,
            max_iters=3
        )
        print(f"[OK] ResearchAgent åˆ›å»ºå®Œæˆ")
        
        # 7. è¿›è¡Œé—®ç­”
        print(f"\næ­¥éª¤ 6: è¿›è¡Œé—®ç­”")
        qas = collect_qa_items_for_sample(sample)
        print(f"å…±æœ‰ {len(qas)} ä¸ªé—®é¢˜éœ€è¦å›ç­”")
        
        # å®šä¹‰å¤„ç†å•ä¸ªé—®é¢˜çš„workerå‡½æ•°
        def process_question(qi_with_index):
            """å¤„ç†å•ä¸ªé—®é¢˜çš„workerå‡½æ•°"""
            i, qi = qi_with_index
            q = qi.get("question") or ""
            gold = qi.get("answer")
            cat = qi.get("category")
            
            print(f"\n--- é—®é¢˜ {i}/{len(qas)} ---")
            print(f"é—®é¢˜: {q}")
            print(f"æ ‡å‡†ç­”æ¡ˆ: {gold}")
            print(f"åˆ†ç±»: {cat}")
            
            if cat == 5:
                return None

            try:
                # ä½¿ç”¨ ResearchAgent è¿›è¡Œç ”ç©¶
                print(f"[é—®é¢˜ {i}] æ­£åœ¨è¿›è¡Œæ·±åº¦ç ”ç©¶...")
                result = research_agent.research(q)
                research_summary = result.integrated_memory
                print(f"[é—®é¢˜ {i}] [OK] ç ”ç©¶å®Œæˆï¼è¿­ä»£æ¬¡æ•°: {len(result.raw_memory.get('iterations', []))}")
                print(f"[é—®é¢˜ {i}] ç ”ç©¶æ‘˜è¦: {research_summary[:200]}...")
                
                # ä¿å­˜ç ”ç©¶è½¨è¿¹
                research_trace = {
                    "question": q,
                    "raw_memory": result.raw_memory,
                    "integrated_memory": result.integrated_memory,
                    "iterations": result.raw_memory.get("iterations", []),
                    "search_plans": result.raw_memory.get("search_plans", []),
                    "reflections": result.raw_memory.get("reflections", [])
                }
                
                # ä¿å­˜å•ä¸ªé—®é¢˜çš„ç ”ç©¶è½¨è¿¹
                trace_file = os.path.join(sample_results_dir, f"research_trace_q{i}.json")
                with open(trace_file, 'w', encoding='utf-8') as f:
                    json.dump(research_trace, f, ensure_ascii=False, indent=2)
                print(f"[é—®é¢˜ {i}] [INFO] ç ”ç©¶è½¨è¿¹å·²ä¿å­˜: {trace_file}")
                
                # åŸºäºç ”ç©¶ç»“æœç”Ÿæˆç­”æ¡ˆï¼ˆæ ¹æ®categoryé€‰æ‹©ä¸åŒpromptï¼‰
                print(f"[é—®é¢˜ {i}] ç”Ÿæˆç­”æ¡ˆ...")
                summary_answer = answer_with_summary(cat, research_summary, q, working_generator)
                
                print(f"[é—®é¢˜ {i}] é¢„æµ‹ç­”æ¡ˆ: {summary_answer}")
                
                qa_result = {
                    "question": q,
                    "gold_answer": gold,
                    "category": cat,
                    "research_summary": research_summary,
                    "summary_answer": summary_answer,
                    "iterations": len(result.raw_memory.get("iterations", [])),
                    "research_trace_file": trace_file
                }
                return qa_result
            
            except Exception as e:
                print(f"[é—®é¢˜ {i}] [ERROR] å¤„ç†é—®é¢˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                qa_result = {
                    "question": q,
                    "gold_answer": gold,
                    "category": cat,
                    "error": str(e)
                }
                return qa_result
        
        # å¤„ç†æ‰€æœ‰é—®é¢˜
        qa_items_with_index = [(i, qi) for i, qi in enumerate(qas, 1)]
        
        print(f"å¼€å§‹ä¸²è¡Œå¤„ç† {len(qa_items_with_index)} ä¸ªé—®é¢˜...")
        
        qa_results = []
        for qa_item in tqdm(qa_items_with_index, desc="å¤„ç†é—®é¢˜"):
            result = process_question(qa_item)
            # è¿‡æ»¤æ‰Noneç»“æœï¼ˆcategory==5çš„é—®é¢˜ï¼‰
            if result is not None:
                qa_results.append(result)
        
        # ä¿å­˜ç»“æœ
        results_file = os.path.join(sample_results_dir, "qa_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(qa_results, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # ä¿å­˜æ‰€æœ‰ç ”ç©¶è½¨è¿¹çš„æ±‡æ€»
        all_research_traces = []
        for i, qa_result in enumerate(qa_results, 1):
            if "research_trace_file" in qa_result:
                trace_file = qa_result["research_trace_file"]
                if os.path.exists(trace_file):
                    with open(trace_file, 'r', encoding='utf-8') as f:
                        trace_data = json.load(f)
                        all_research_traces.append({
                            "question_index": i,
                            "question": qa_result["question"],
                            "category": qa_result["category"],
                            "research_trace": trace_data
                        })
        
        if all_research_traces:
            traces_summary_file = os.path.join(sample_results_dir, "all_research_traces.json")
            with open(traces_summary_file, 'w', encoding='utf-8') as f:
                json.dump(all_research_traces, f, ensure_ascii=False, indent=2)
            print(f"[OK] æ‰€æœ‰ç ”ç©¶è½¨è¿¹æ±‡æ€»å·²ä¿å­˜åˆ°: {traces_summary_file}")
        
        # æ€»ç»“
        print(f"\n{'='*60}")
        print("å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ ·æœ¬ID: {sample_id}")
        print(f"ä¼šè¯æ•°: {len(session_chunks)}")
        print(f"è®°å¿†æ‘˜è¦æ•°: {len(final_state.abstracts)}")
        print(f"å¤„ç†é—®é¢˜æ•°: {len(qa_results)}")
        print(f"ç ”ç©¶è½¨è¿¹æ–‡ä»¶æ•°: {len(all_research_traces)}")
        print(f"ç»“æœä¿å­˜åˆ°: {sample_results_dir}")
        print(f"  - QAç»“æœ: qa_results.json")
        print(f"  - è®°å¿†çŠ¶æ€: memory_state.json")
        print(f"  - ç ”ç©¶è½¨è¿¹æ±‡æ€»: all_research_traces.json")
        print(f"  - å•ä¸ªç ”ç©¶è½¨è¿¹: research_trace_q*.json")
        
        return qa_results
        
    except Exception as e:
        error_msg = f"å¤„ç†æ ·æœ¬ {sample_index} æ—¶å‡ºé”™: {str(e)}"
        print(f"ERROR: {error_msg}")
        import traceback
        traceback.print_exc()
        return []


# ========== ä¸»å‡½æ•° ==========

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="GAM æ¡†æ¶ + LoCoMo æ•°æ®é›†æµ‹è¯•")
    parser.add_argument("--data", type=str, default="/path/to/locomo/dataset.json", 
                        help="LoCoMo æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--outdir", type=str, default="./results/locomo",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start-idx", type=int, default=0, help="å¼€å§‹æ ·æœ¬ç´¢å¼•")
    parser.add_argument("--end-idx", type=int, default=None, help="ç»“æŸæ ·æœ¬ç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ ·æœ¬")
    
    # Memory Generator é…ç½®
    parser.add_argument("--memory-api-key", type=str, default="empty", help="Memory æ¨¡å‹ API Key")
    parser.add_argument("--memory-base-url", type=str, default="https://api.openai.com/v1", help="Memory æ¨¡å‹ Base URL")
    parser.add_argument("--memory-model", type=str, default="gpt-4o-mini", help="Memory æ¨¡å‹åç§°")
    
    # Research Generator é…ç½®
    parser.add_argument("--research-api-key", type=str, default="empty", help="Research æ¨¡å‹ API Key")
    parser.add_argument("--research-base-url", type=str, default="https://api.openai.com/v1", help="Research æ¨¡å‹ Base URL")
    parser.add_argument("--research-model", type=str, default="gpt-4o-mini", help="Research æ¨¡å‹åç§°")
    
    # Working Generator é…ç½®
    parser.add_argument("--working-api-key", type=str, default="empty", help="Working æ¨¡å‹ API Key")
    parser.add_argument("--working-base-url", type=str, default="https://api.openai.com/v1", help="Working æ¨¡å‹ Base URL")
    parser.add_argument("--working-model", type=str, default="gpt-4o-mini", help="Working æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("GAM æ¡†æ¶ + LoCoMo æ•°æ®é›†æµ‹è¯•")
    print("=" * 60)
    print(f"æ•°æ®é›†: {args.data}")
    print(f"è¾“å‡ºç›®å½•: {args.outdir}")
    print(f"æ ·æœ¬èŒƒå›´: {args.start_idx} åˆ° {args.end_idx-1 if args.end_idx else 'å…¨éƒ¨'} (å…± {args.end_idx - args.start_idx if args.end_idx else 'å…¨éƒ¨'} ä¸ªæ ·æœ¬)")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    samples = load_locomo(args.data)
    print(f"å…±åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
    
    # é‡æ–°è®¾ç½®ç»“æŸç´¢å¼•ï¼ˆåœ¨åŠ è½½æ•°æ®åï¼‰
    if args.end_idx is None:
        args.end_idx = len(samples)
    
    print(f"å®é™…å¤„ç†èŒƒå›´: {args.start_idx} åˆ° {args.end_idx-1} (å…± {args.end_idx - args.start_idx} ä¸ªæ ·æœ¬)")
    
    # éªŒè¯ç´¢å¼•èŒƒå›´
    if args.start_idx < 0 or args.start_idx >= len(samples):
        print(f"é”™è¯¯: å¼€å§‹æ ·æœ¬ç´¢å¼• {args.start_idx} è¶…å‡ºèŒƒå›´ (æ€»æ ·æœ¬æ•°: {len(samples)})")
        return
    
    if args.end_idx > len(samples):
        print(f"è­¦å‘Š: ç»“æŸæ ·æœ¬ç´¢å¼• {args.end_idx} è¶…å‡ºèŒƒå›´ï¼Œè°ƒæ•´ä¸º {len(samples)}")
        args.end_idx = len(samples)
    
    if args.start_idx >= args.end_idx:
        print(f"é”™è¯¯: å¼€å§‹ç´¢å¼• {args.start_idx} å¿…é¡»å°äºç»“æŸç´¢å¼• {args.end_idx}")
        return
    
    # é¡ºåºå¤„ç†æ¯ä¸ªæ ·æœ¬
    sample_indices = list(range(args.start_idx, args.end_idx))
    
    print(f"å°†é¡ºåºå¤„ç† {len(sample_indices)} ä¸ªæ ·æœ¬...")
    
    all_results = []
    
    # é¡ºåºå¤„ç†æ¯ä¸ªæ ·æœ¬
    for sample_idx in tqdm(sample_indices, desc="å¤„ç†æ ·æœ¬"):
        sample = samples[sample_idx]
        print(f"\n{'='*80}")
        print(f"å¼€å§‹å¤„ç†æ ·æœ¬ {sample_idx}/{len(samples)-1} (èŒƒå›´: {args.start_idx}-{args.end_idx-1})")
        print(f"{'='*80}")
        
        try:
            results = process_sample(
                sample, 
                sample_idx, 
                args.outdir,
                args.memory_api_key,
                args.memory_base_url,
                args.memory_model,
                args.research_api_key,
                args.research_base_url,
                args.research_model,
                args.working_api_key,
                args.working_base_url,
                args.working_model
            )
            print(f"[OK] æ ·æœ¬ {sample_idx} å¤„ç†å®Œæˆ")
            all_results.extend(results)
        except Exception as e:
            print(f"[ERROR] æ ·æœ¬ {sample_idx} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜æ‰€æœ‰ç»“æœæ±‡æ€»
    if all_results:
        summary_file = os.path.join(args.outdir, f"batch_results_{args.start_idx}_{args.end_idx-1}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] æ‰¹é‡ç»“æœæ±‡æ€»å·²ä¿å­˜: {summary_file}")
        
        # è®¡ç®—æŒ‡æ ‡
        print(f"\n{'='*60}")
        print("å¼€å§‹è®¡ç®—æŒ‡æ ‡...")
        print(f"{'='*60}")
        
        # è®¡ç®— summary_answer çš„æŒ‡æ ‡
        pred_key = "summary_answer"
        pred_field = "answer"
        
        print(f"\n# LoCoMo Metrics for pred_key='{pred_key}', pred_field='{pred_field}'")
        summary, details = compute_metrics_by_category(all_results, pred_key=pred_key, pred_field=pred_field)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for r in summary:
            print(f"Category {r['category']}: n={r['count']}, F1_avg={r['F1_avg']:.4f}, BLEU1_avg={r['BLEU1_avg']:.4f}")
        
        # è®¡ç®—æ•´ä½“å¹³å‡æŒ‡æ ‡
        all_f1_scores = [row["F1"] for row in details]
        all_bleu1_scores = [row["BLEU1"] for row in details]
        overall_f1_avg = sum(all_f1_scores) / len(all_f1_scores) if all_f1_scores else 0.0
        overall_bleu1_avg = sum(all_bleu1_scores) / len(all_bleu1_scores) if all_bleu1_scores else 0.0
        
        print(f"\næ•´ä½“ç»Ÿè®¡:")
        print(f"æ€»é—®é¢˜æ•°: {len(all_results)}")
        print(f"æ•´ä½“å¹³å‡ F1: {overall_f1_avg:.4f}")
        print(f"æ•´ä½“å¹³å‡ BLEU1: {overall_bleu1_avg:.4f}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶ï¼ˆç±»ä¼¼ hotpotqa_test.pyï¼‰
        statistics = {
            "total_samples": args.end_idx - args.start_idx,
            "total_questions": len(all_results),
            "overall_f1_avg": overall_f1_avg,
            "overall_bleu1_avg": overall_bleu1_avg,
            "by_category": summary,
            "details": details,
            "start_idx": args.start_idx,
            "end_idx": args.end_idx - 1
        }
        
        stats_file = os.path.join(args.outdir, f"batch_statistics_{args.start_idx}_{args.end_idx-1}.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        print(f"\næŒ‡æ ‡ç»“æœå·²ä¿å­˜åˆ°: {stats_file}")
    
    print(f"\n{'='*60}")
    print("[OK] æ‰¹é‡æµ‹è¯•å®Œæˆï¼")
    print(f"å¤„ç†æ ·æœ¬æ•°: {args.end_idx - args.start_idx}")
    print(f"æˆåŠŸå¤„ç†: {len(all_results)} ä¸ªé—®é¢˜")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()

